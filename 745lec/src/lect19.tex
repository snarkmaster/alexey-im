\documentclass[11pt]{article}
\usepackage{slashbox}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[all]{xy}
\usepackage{epsfig}
\usepackage{pstricks}

\newcommand{\handout}[5]{
  \noindent
  \begin{center}
  \framebox{
    \vbox{
      \hbox to 5.78in { {\bf 18.745 Introduction to Lie Algebras } \hfill #2 }
      \vspace{4mm}
      \hbox to 5.78in { {\Large \hfill #5  \hfill} }
      \vspace{2mm}
      \hbox to 5.78in { {\em #3 \hfill #4} }
    }
  }
  \end{center}
  \vspace*{4mm}
}

\newcommand{\lecture}[4]{\handout{#1}{#2}{#3}{Scribe: #4}{Lecture #1}}


\DeclareSymbolFont{AMSb}{U}{msb}{m}{n}
\DeclareMathSymbol{\N}{\mathbin}{AMSb}{"4E}
\DeclareMathSymbol{\Z}{\mathbin}{AMSb}{"5A}
\DeclareMathSymbol{\R}{\mathbin}{AMSb}{"52}
\DeclareMathSymbol{\Q}{\mathbin}{AMSb}{"51}
\DeclareMathSymbol{\I}{\mathbin}{AMSb}{"49}
\DeclareMathSymbol{\C}{\mathbin}{AMSb}{"43}
\DeclareMathSymbol{\F}{\mathbin}{AMSb}{"46}

\newcommand{\sll}{\mathfrak{sl}}
\newcommand{\so}{\mathfrak{so}}
\newcommand{\spp}{\mathfrak{sp}}
\newcommand{\gl}{\mbox{gl}}
\newcommand{\GL}{\mbox{GL}}
\newcommand{\tr}{\mbox{tr\ }}
\newcommand{\Mat}{\mbox{Mat}}
\newcommand{\Lie}{\mbox{Lie}}
\newcommand{\Der}{\mbox{Der\ }}
\newcommand{\End}{\mbox{End\ }}
\newcommand{\ad}{\mbox{ad\ }}
\newcommand{\im}{\mbox{im\ }}
\newcommand{\Ker}{\mbox{ker\ }}

\newcommand{\g}{\mathfrak{g}}
\newcommand{\h}{\mathfrak{h}}
\newcommand{\m}{\mathfrak{m}}
\newcommand{\n}{\mathfrak{n}}
\newcommand{\He}{\mathfrak{H}}

\newcommand{\sk}{\vspace*{1em}}

\newtheorem{defn}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}
\newtheorem{proof}{Proof}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}

% 1-inch margins, from fullpage.sty by H.Partl, Version 2, Dec. 15, 1988.
\topmargin 0pt
\advance \topmargin by -\headheight
\advance \topmargin by -\headsep
\textheight 8.9in
\oddsidemargin 0pt
\evensidemargin \oddsidemargin
\marginparwidth 0.5in
\textwidth 6.5in

\parindent 0in
\parskip 1.5ex
%\renewcommand{\baselinestretch}{1.25}

\begin{document}

\lecture{19 --- November 16, 2004}{Fall 2004}{Prof.\ Victor Ka\v{c}}{Patrick Lam}

We have been proving the following,

\begin{theorem}
(Classification theorem)
Let $\g$ be a finite-dimensional semisimple Lie algebra over an
algebraically closed field of characteristic 0.  Then $\g$ is
isomorphic to a direct sum of simple Lie algebras,
and a complete and non-redundant list of the latter is as follows:
\[ \sll_n(\F)\ (n \ge 2), \so_n(\F)\ (n \ge 7), \spp_n(\F)\ (n \ge 4, \mbox{even}),
E_6, E_7, E_8, F_4, G_2. \]
\end{theorem}

\newcommand{\ddd}{-\!\!\!}
The strategy of the proof is given in the following diagram.
\begin{eqnarray*}
 \g& \stackrel{\mbox{\scriptsize choose $\h$}}{\ddd \ddd \ddd \longrightarrow} 
\begin{array}{c}
\mathop{\Delta}\\
{\mbox{\scriptsize root system}}
\end{array} = 
\mbox{\scriptsize $\begin{array}{c}\mbox{abstract} \\ \mbox{root system}\end{array}$} 
\stackrel{\mbox{\scriptsize  choose $f \in V^*$}}{\ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \longrightarrow} \Pi \longrightarrow 
\begin{array}{c}
\mathop{A}\\
{\mbox{\scriptsize Cartan matrix}}
\end{array} =
&\mbox{abstract}\\[-1em]
\uparrow &&\mbox{Cartan}\\[-0.5em]
\g(A)&\longleftarrow\!\!\!\ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd\ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd \ddd  -&\mbox{matrix\ } A
\end{eqnarray*}

First construct the following set of generators $E_i, F_i, H_i\ (i=1,
\ldots, r = \dim \h = \mbox{rank }(\g))$ as follows: let $\alpha_i$
be a simple root, and choose $E_i \in \g_{\alpha_i}, F_i \in \g_{-\alpha_i}$
such that $[E_i, F_i] = \frac{2 \nu^{-1}(\alpha_i)}{K(\alpha_i, \alpha_j)}$
(recall that $[\g_{\alpha_i}, \g_{-\alpha_i}] = \F \nu^{-1}(\alpha_i),
\g_{\alpha_i} = \F E_i, \g_{-\alpha_i} = \F F_i$.)  Then we have:
\[ [H_i, H_j] = 0, [H_i, E_j] = a_{ij} E_j, [H_i, F_j] = -a_{ij} F_j,
[E_i, F_j] = \delta_{ij} H_i \eqno{(*)} \]
where $a_{ij} := \frac{2K(\alpha_i, \alpha_j)}{K(\alpha_i, \alpha_i)}$ are
the entries of the Cartan matrix $A = (a_{ij})$.

To check these: $[H_i, E_j] = \alpha_j(H_i) E_j = \frac{2 \alpha_j (\nu^{-1}(\alpha_i))}{K(\alpha_i, \alpha_i)} = \frac{2K(\alpha_j, \alpha_i)}{K(\alpha_i, \alpha_i)} = a_{ij}$; $[H_i, F_j] = -a_{ij} F_j$ is clear; $[E_i, F_i]$ has been
checked.  $[E_i, F_j] = 0$ because it belongs to $\g_{\alpha_i - \alpha_j}$,
so it is not a root by part (a) of the Theorem in Lecture 17.

Next, we denote by $\n_+$ (respectively $\n_-$) the subalgebra of
$\g$ generated by all $E_i$s (respectively $F_i$s).  Then
\[ \n_+ = \bigoplus_{\alpha \in \Delta_+} \g_\alpha, \quad
   \n_- = \bigoplus_{\alpha \in -\Delta_+} \g_\alpha. \]
Indeed, let $\alpha \in \Delta_+ \setminus \Pi$.  Then
by part (c) of the Theorem from Lecture 17, $\alpha - \alpha_i 
\in \Delta_+$ for some simple root $\alpha_i$, hence $\g_\alpha =
[\g_{\alpha - \alpha_i}, E_i]$ since $\dim \g_{\alpha} = 1$.
$[\g_\alpha, \g_\beta] = \g_{\alpha+\beta}$ if $\alpha, \beta,
\alpha+\beta \in \Delta$ by part (d) of Theorem 3 from Lecture 13.

This proves that all $\g_\alpha$, with $\alpha \in \Delta_+$, are in
$\n_+$.  Likewise all $\g_{-\alpha} (\alpha \in \Delta_+)$ are in
$\n_-$.  Note that we have the so-called triangular decomposition
$\g = \n_- \oplus \h \oplus \n_+$ (a direct sum as vector spaces).

\sk\noindent
{\bf Exercise 19.1}. (a) Show that for $\sll_n$, $\n_{\pm} = $ 
strictly {\scriptsize $\begin{array}{c}\mbox{upper} \\ \mbox{lower}\end{array}$}
triangular matrices, if $\h = $ diagonal matrices, and if $f$ chosen
as we did.  (b) Find the triangular decompositions for $\so_n$ and
$\spp_n$.

\sk\noindent {\bf Solution.} (a) In Lecture 14, we noted that 
\[ \sll_n(\F) = \h \oplus \left(\bigoplus_{\mbox{\scriptsize
    $\begin{array}{c}i,j=1\\i \neq j\end{array}$}}^n \F E_{ij}\right), \]
where $\h$ was the diagonal matrices, and $\F E_{ij}$ is the root
space attached to the root $\varepsilon_i - \varepsilon_j$.  We need
to distinguish the positive roots from the negative roots; the $f$ we
have chosen gives positive roots for each $E_{ij}$ where $i > j$. The
direct sum of the spaces $\F E_{ij}$ where $i > j$ is indeed the
desired space $\n_+$ of strictly upper triangular matrices, and $\n_-$
is the space of strictly lower triangular matrices, showing the triangular
decomposition.


(b) For $\so_n(\F)$, we have
{\scriptsize \[ \h = \left\{ \left( \begin{array}{ccccc}
a_1 & & & & 0 \\ & a_2 \\ & & \ddots \\
& & & -a_2 \\ 0 & & & & -a_1 \end{array} \right) \right\}. \] }
and $\so_n(\F) = \h \oplus \left( \bigoplus_{i,j} \F F_{ij} \right)$,
where $F_{ij} = E_{ij} - E_{n+1-j, n+1-i}$.  Then the positive roots
are $\varepsilon_i \pm \varepsilon_j (i < j), \varepsilon_i$.
Hence the space $\n_+$ is the set of matrices $A$ such that
$A' = -A$ which are strictly upper-triangular, while the $\n_-$ is the set
of matrices $A$ such that $A' = -A$ which are strictly lower-triangular.

For $\spp_{n}(\F)$, we have the same Cartan subalgebra $\h$; this
time, the $F_{ij} = E_{ij} - E_{n+1-j, n+1-i}$ for $1 \le i, j \le r
(i \neq j)$ and $F_{ij} = E_{ij} + E_{n+1-j, n+1-i}$ ($1 \le i \le r,
r+1 \le j \le n$.  The corresponding root vectors are
$\Delta_{\spp_{2r}} = \{ \varepsilon_i - \varepsilon_j (i, j = 1,
\ldots, r, i \neq j), \varepsilon_i + \varepsilon_j,
-\varepsilon_i-\varepsilon_j (i, j = 1, \ldots, r)\}$; the positive
roots are now $\varepsilon_i \pm \varepsilon_j (i < j)$ and $2
\varepsilon_i$ if $n$ is even.  Finally, $\n_+$ is the 
set of upper triangular matrices (in block form) 
$\left( \begin{array}{cc} a & b\\ c & d \end{array} \right)$ such
that $d = -a', b = b', c = c'$, where $'$ denotes transposition
with respect to the antidiagonal, and $\n_-$ is the set of
lower triangular matrices which satisfy this condition.

\begin{remark} Any non-zero ideal of $\g$ has a non-zero 
intersection with $\h$. \end{remark}

\begin{lemma} Let $\h$ be a finite-dimensional abelian Lie algebra,
and let $\pi$ be a diagonalizable representation of $\h$ in a vector
space $V$ (not necessarily finite-dimensional), i.e. 
$V = \bigoplus_{\lambda \in \h^*} V_\alpha$, where $V_\lambda =
\{ v \in V \mid \pi(h)v = \lambda(h) v, h \in \h \}$.
Then for any $\pi(\h)$-invariant subspace $U \subset V$,
we have $U = \bigoplus_{\lambda \in \h^*} (U \cap V_\lambda)$.
\end{lemma}

\paragraph{Proof.} Take $u \in U$ and write $u = \sum_\lambda v_{\lambda}$,
where $v_\lambda \in V_\lambda$.  Take $h \in \h$ such that 
$\lambda_1(h) \neq \lambda_2(h)$
(scale so that $\lambda_1(h) = 1$.)  Then $U \ni \pi(h) =
\lambda_1(h) v_{\lambda_1} + \lambda_2(h) v_{\lambda_2} + \cdots$.
Hence 
\[U - \pi(h)u = (1-\lambda_2(h)) v_{\lambda_2} + (1-\lambda_3(h))
v_{\lambda_3} + \cdots,\]
and we may apply induction on the number of summands in $u$.
$\Box$  

\paragraph{Proof of the Remark.} If $I$ is a non-zero ideal of $\g$
which intersects $\h$ trivially, then by the lemma, $\g_\alpha \subset
I$ for some root $\alpha$.  But then $[\g_\alpha, g_{-\alpha}] =
\F \nu^{-1}(\alpha) \subset I$, a contradiction. $\Box$

Let $\tilde{\g}(A)$ be the Lie algebra on generators $E_i, F_i, H_i
\ (i=1, \ldots, r)$ subject to relations (*). 

\sk\noindent
{\bf Exercise 19.2}. Show that $\tilde{\g}((2)) = \sll_2(\F)$
but $\tilde{\g}\left(\left(\begin{array}{rr} 2 & -1 \\ -1 & 2 \end{array}
\right) \right)$ is infinite dimensional.  Find the elements of
$\mathcal{J}\left(\begin{array}{rr} 2 & -1 \\ -1 & 2 \end{array}
\right)$.

\sk\noindent {\bf Solution.}
For the case $\tilde{\g}((2)) = \sll_2(\F)$, we have $r=1$, giving the
generators $E_1, F_1, H_1$ with the relationships $[E_1, F_1] = H_1;
[H_1, H_1] = 0; [H_1, E_1] = 2E_1; [H_1, F_1] = -2F_1$.
Let 
\[
H_1 = \left( \begin{array}{cc} 1 & 0 \\ 0 & -1 \end{array} \right);
E_1 = \left( \begin{array}{cc} 0 & 1 \\ 0 & 0 \end{array} \right);
F_1 = \left( \begin{array}{cc} 0 & 0 \\ 1 & 0 \end{array} \right);
\]
simple calculations show that these relationships are satisfied,
and these matrices generate $\sll_2(\F)$.


\sk\noindent To show that $\tilde{\g}\left(\left(\begin{array}{rr} 2 & -1 \\ -1 & 2 \end{array}
\right) \right)$ is infinite-dimensional, we note that $\tilde{\n}_+$ 
and $\tilde{\n}_-$ are freely generated over the generators $E_1, E_2$,
which is an infinite-dimensional space.  Since $\tilde{\g} = \tilde{\n}_+
\oplus \tilde{\h} \oplus \tilde{\n}_-$, it is also infinite-dimensional.

\sk\noindent The maximal ideal $\mathcal{J}\left(\begin{array}{rr} 2 & -1 \\ -1
    & 2 \end{array} \right)$ is the ideal containing the inverse
images of the triple commutators $E_1E_2E_1$ and $E_2E_1E_2$, as well
as $F_1F_2F_1$ and $F_2F_1F_2$. $\Box$


\begin{lemma} (a) $\tilde{\g}(A) = \tilde{\n}_- \oplus \h \oplus
\tilde{\n}_+,$ where $\tilde{\n}_+$ (resp. $\tilde{\n}_-$)
is generated by the $E_i$s (resp. $F_i$s) and $\h = \mbox{span of $H_i$s}$.
(b) $\tilde{\g}(A)$ has a maximal ideal $\mathcal{J}(A)$ among ideals
intersecting $\h$ trivially. \end{lemma}

\paragraph{Proof.} ({\bf Exercise 19.3.}) First we show by induction on $n$ that any commutator
of length $n$:
\[ [ [b_{i_1}, b_{i_2}], \ldots, b_{i_n}] 
\mbox{\quad (where $b_{i_k} \in \{ E_i, F_i, H_i\}$),} \]
lie in either $\n_+$ or
in $\n_-$ or in $\h$.

\paragraph{Solution.}
The base case is obvious.  Now assume 
$b = [ [b_{i_1}, b_{i_2}], \ldots, b_{i_n}]$ is in
$\n_+$, $\n_-$ or $\h$; we show
that $b' = [[[b_{i_1}, b_{i_2}], \ldots, b_{i_n}], b_{i_{n+1}}]$
is in $\n_+$, $\n_-$ or $\h$.  The following table summarizes
the possible values of the $n+1$-length commutator $b'$, ignoring
irrelevant constant factors in front of the $b$.

\begin{center}
\begin{tabular}{l|ccc}
\backslashbox{$b$}{$b_{i_{n+1}}$} & $E_i$ & $F_i$ & $H_i$ \\ \hline
$E_j$ & 0 & $\delta_{ji} H_j$ & $-a_{ij} E_j$ \\
$F_j$ & $-\delta_{ij} H_i$ & 0 & $-a_{ij} F_j$ \\
$H_j$ & $a_{ji} E_i$ & $-a_{ji} F_i$ & 0 \\
\end{tabular}
\end{center}

Recalling that $\n_+$ is defined to be the subalgebra generated by the
$E_i$, that $\n_-$ is generated by the $F_i$ and that $\h$ is
generated by the $H_i$, we observe that $b'$, the $n+1$-length
commutator, always belongs to one of $\n_+, \n_-$ or $\h$ if all
the $b_{i_k}$s are $E_i, F_i$ or $H_i$. $\Box$

Hence $\tilde{\g}(A) = \tilde{\n}_- + \h + \tilde{\n_+}$.  This sum is
direct since $\tilde{\n}_+ = \bigoplus \tilde{\g}(A)_\alpha, \alpha =
\sum_{i=1}^n k_i \alpha_{ij}, k_i \in \Z_i$, not all 0,
and similarly for $\tilde{\n}_-$, but $\h = \tilde{\g}(A)_0$.  
(This proves (a).)  

But by Lemma 1, any ideal $I$ of $\tilde{\g}(A)$ is of the form
$I = I_- \oplus I_0 \oplus I_+$, where $I_\pm \subset \tilde{\n}_\pm$,
$I_0 \subset \h$.  So if $I \cap \h = 0$, then $I = I_- \oplus I_+,
I_\pm \subset \tilde{\n}_\pm$.  Hence (b) follows, since
$\mathcal{J}(A)$, as a sum of all ideals intersecting $\h$ trivally,
is a proper maximal ideal.  We let $\g(A) = \tilde{\g}(A) / \mathcal{J}(A)$.
Now consider the surjective homomorphism $\tilde{\g}(A) 
\stackrel{\varphi}{\rightarrow} \g$ defined by
$\varphi(E_i) = E_i, \varphi(F_i) = F_i, \varphi(H_i) = H_i$ 
(which generate $\g$).  Moreover, $\mathcal{J}(A) \subset \ker \varphi$,
otherwise $\varphi(\mathcal{J}(A))$ contradicts Remark 1, and furthermore,
$\mathcal{J}(A) = \ker \varphi$ since $\mathcal{J}(A)$ is maximal.  Hence
we have an isomorphism $\g(A) \tilde{\rightarrow} \g$.
(Note that $\varphi$ is mapping the infinite-dimensional $\n_+$ and 
$\n_-$ to finite-dimensional $\tilde{\n}_+$ and $\tilde{\n}_-$). $\Box$

So, if we know that $\g$ with a given root system exists, like $\g =
\sll_n, \so_n, \mbox{ or } \spp_n$, then $\g(A) = \g$ is the Lie
algebra with this root system.  We have proved that any simple Lie
algebra over $\F$ is isomorphic to $\sll_n(\F), \so_n(\F),
\spp_n(\F)$, or possibly the simple Lie algebras whose root systems
are $\Delta_{E_i}, i = 6, 7, 8, \Delta_{F_4}, \Delta_{G_2}$, called the
exceptional Lie algebras.  So it remains to prove the existence of the
latter.  Note that we have the same Cartan matrices in the following 
cases:
\begin{eqnarray*}
A_1 = B_1 = C_1 && \mbox{(1-d root systems are identical)}\\
B_2 = C_2 && 
\xymatrix{ {\bigcirc} \ar@{=>}[r]& {\bigcirc}} \quad
\xymatrix{ {\bigcirc} \ar@{<=}[r]& {\bigcirc}}\\
D_3 = A_3 &&
\xymatrix{ {\bigcirc} \ar@{-}[r]& {\bigcirc}\ar@{-}[r]& {\bigcirc}}
\\
D_2 = A_1 \oplus A_1
\end{eqnarray*}
or, on the level of Lie algebras, $\sll_2(\F) \simeq \so_3(\F) 
\simeq \spp_2(\F); \so_5(\F) \simeq \spp_4(\F); 
\so_6 (\F) \simeq \sll_4(\F); \so_4 \simeq \sll_2 (\F)
\oplus \sll_2(\F)$.

Now we want to construct the simple Lie algebras from their
root systems.  (We will carry out an explicit construction; this construction
doesn't show uniqueness.)

First we consider the simply-laced case $A = A^T$, i.e. all 
roots have the same length.  Let $(V, \Delta)$ be a simply-laced
root system, with $(\cdot, \cdot)$ such that $(\alpha, \alpha) = 2$
for any root $\alpha$, and let $Q = \Z \Delta$ be the root lattice.
Then $\Delta = \{ \alpha \in Q \mid (\alpha, \alpha) = 2 \}$, in
all examples $A_r, D_r, E_6, E_7, E_8$; the first two are known to
exist, whereas the last three are new.  

Consider the following space over $\F$: $\g = \h \oplus (\bigoplus_{\alpha
\in \Delta} \F E_\alpha),$ where $\h = \F \otimes_\Q (\Q \Delta)$
and $E_\alpha$ are some non-zero vectors.  Extend the bilinear
form from $(\cdot, \cdot)$ on $V$ to $\h$ by bilinearity. Define brackets on
$\g$ as follows:
\begin{enumerate}
\item[(i)] $[h, h'] = 0$ for $h, h' \in \h$.
\item[(ii)] $[h, E_\alpha] = (h, \alpha) E_\alpha$
\item[(iii)] $[E_\alpha, E_{-\alpha}] = -\alpha$ (for convenience)
\item[(iv)] $[E_\alpha, E_\beta] = 0$ if $\alpha + \beta \not\in \Delta \cup \{ 0 \}$
\item[(v)] $[E_\alpha, E_\beta] = \varepsilon(\alpha, \beta)
  E_{\alpha+\beta}$ if $\alpha+\beta \in \Delta$.
\end{enumerate}

Next time, we will study how to construct $\varepsilon$ so that the
Jacobi identity holds.  To do this, we have to check the Jacobi identities
of any triple of distinct basis elements of $\g$:
\begin{enumerate}
\item[(i)] If this triple is $h, h', h'' \in \h$, the identity is obvious.
\item[(ii)] If $h, h', E_\alpha$ or $h, E_\alpha, E_\beta$,
the Jacobi identity holds for any choice of $\varepsilon(\alpha, \beta)$
({\bf Exercise 19.4}).
\item[(iii)] (next time) If $E_\alpha, E_\beta, E_\gamma$, we will have to choose
$\varepsilon$ appropriately.
\end{enumerate}

\paragraph{Solution.} A straightforward calculation shows the Jacobi identity:
\begin{eqnarray*}
[h, [h', E_\alpha]] + [h', [E_\alpha, h]] + [E_\alpha, [h, h']] 
&=& [h, [h', E_\alpha]] + [h', [E_\alpha, h]] \\
&=& [h, (h', \alpha) E_\alpha] + [h', -(h, \alpha) E_\alpha] \\
&=& (h', \alpha)[h, E_\alpha] - (h, \alpha) [h', E_\alpha] \\
&=& (h', \alpha)(h, \alpha) E_\alpha - (h, \alpha)(h', \alpha) E_\alpha \\
&=& 0
\end{eqnarray*}
and
\begin{eqnarray*}
&& [h, [E_\alpha, E_{\beta} ] ] + [E_\alpha, [E_\beta, h]] + 
    [E_\beta, [h, E_\alpha]] \\
&=& (\alpha+\beta, h) [E_\alpha, E_\beta] - (\beta, h)
[E_\alpha, E_\beta] + (\alpha, h) [E_\beta, E_\alpha] \\
&=& (\alpha+\beta, h) [E_\alpha, E_\beta] - (\beta, h)
[E_\alpha, E_\beta] - (\alpha, h) [E_\alpha, E_\beta] \\
&=& 0.
\end{eqnarray*}
$\Box$

\end{document}



